{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# local files\n",
    "from src.util.data_handling.data_loader import save_as_pickle, load_dataset\n",
    "from src.util.data_handling.string_generator import str_seq_to_num_seq, ALPHABETS\n",
    "from src.data.edit_distance import cross_distance_matrix_threads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasta(source_sequences):\n",
    "\n",
    "    # load sequences\n",
    "    with open(source_sequences, 'rb') as f:\n",
    "        L = f.readlines()\n",
    "        \n",
    "    # store sequences in dictionary\n",
    "    length = 0\n",
    "    id_to_str_seq = {}\n",
    "    for i in range(len(L) // 2):\n",
    "        id_, l = L[2 * i].decode('UTF-8')[1:].strip(), L[2 * i + 1].decode('UTF-8').strip()        \n",
    "        id_to_str_seq[id_] = l\n",
    "        length = max(len(l), length)\n",
    "        \n",
    "    return id_to_str_seq, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ids(id_to_str_seq, split_to_size):\n",
    "    \n",
    "    ids = list(id_to_str_seq.keys())\n",
    "    split_to_ids = {}\n",
    "    cum_sum = 0\n",
    "    \n",
    "    for name, size in split_to_size.items():\n",
    "        split_to_ids[name] = ids[cum_sum: cum_sum + size]\n",
    "        cum_sum += size\n",
    "        \n",
    "    return split_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_distances(str_seqs, length, alphabet, n_thread):\n",
    "    \n",
    "    distances_matrix = cross_distance_matrix_threads(str_seqs, str_seqs, n_thread)\n",
    "    sequences_matrix = [str_seq_to_num_seq(s, length=length, alphabet=alphabet) for s in str_seqs]\n",
    "    \n",
    "    distances_matrix = torch.tensor(distances_matrix).float()\n",
    "    sequences_matrix = torch.tensor(sequences_matrix).long()\n",
    "    \n",
    "    return distances_matrix, sequences_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance_approximation_data(split_to_str_seqs, n_thread, alphabet, length):\n",
    "    \n",
    "    # initial values\n",
    "    sequences = {}\n",
    "    distances = {}\n",
    "    \n",
    "    # compute edit distance and labels\n",
    "    for split, str_seqs in split_to_str_seqs.items():\n",
    "        \n",
    "        str_seqs = str_seqs[:10]\n",
    "        distances_matrix, sequences_matrix = get_sequences_distances(str_seqs, length, alphabet, n_thread)\n",
    "        print('Shapes: {} distances {} {} sequences {}\\n'.format(split, distances_matrix.shape, split, sequences_matrix.shape))\n",
    "        \n",
    "        sequences[split] = sequences_matrix\n",
    "        distances[split] = distances_matrix\n",
    "        \n",
    "    return sequences, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_string_retrieval_data(split_to_str_seqs, n_thread, alphabet, length):\n",
    "\n",
    "    # load data\n",
    "    str_references = split_to_str_seqs['ref']\n",
    "    str_queries = split_to_str_seqs['query']\n",
    "    n_queries = len(split_to_str_seqs['query'])\n",
    "\n",
    "    # convert string sequence to numerical sequence\n",
    "    references = [str_seq_to_num_seq(s, length=length, alphabet=alphabet) for s in str_references]\n",
    "    queries = [str_seq_to_num_seq(s, length=length, alphabet=alphabet) for s in str_queries]\n",
    "\n",
    "    # compute distances and find reference with minimum distance\n",
    "    distances = cross_distance_matrix_threads(str_references, str_queries, n_thread)\n",
    "    minimum = np.min(distances, axis=0, keepdims=True)\n",
    "\n",
    "    # queries are only valid if there is a unique answer (no exaequo)\n",
    "    counts = np.sum((minimum+0.5 > distances).astype(float), axis=0)\n",
    "    valid = counts == 1\n",
    "    labels = np.argmin(distances, axis=0)[valid][:n_queries]\n",
    "\n",
    "    # convert to torch\n",
    "    references = torch.from_numpy(np.asarray(references)).long()\n",
    "    queries = torch.from_numpy(np.asarray(queries)[valid][:n_queries]).long()\n",
    "    labels = torch.from_numpy(labels).float()\n",
    "    print('Shapes: References {} Queries {} Labels {}'.format(references.shape, queries.shape, labels.shape))\n",
    "\n",
    "    return references, queries, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(split_to_size, source_sequences, alphabet, n_thread, outdir, compute_eda_data=True, compute_csr_data=True):\n",
    "\n",
    "    # initial values\n",
    "    filenames = ['{}/{}.pickle'.format(outdir, suffix) for suffix in ['auxillary_data', 'sequences_distances', 'closest_strings']]\n",
    "    \n",
    "    # load and split data\n",
    "    id_to_str_seq, length = load_fasta(source_sequences)\n",
    "    split_to_ids = split_ids(id_to_str_seq, split_to_size)\n",
    "    save_as_pickle((id_to_str_seq, split_to_ids, alphabet, length), filenames[0])\n",
    "    \n",
    "    # seperate data by task: edit distance approximation (eda) and closest string retrival (csr)\n",
    "    eda_split_to_str_seqs = {split: [id_to_str_seq[_id] for _id in split_to_ids[split]] for split in ['train', 'val', 'test']}\n",
    "    csr_split_to_str_seqs = {split: [id_to_str_seq[_id] for _id in split_to_ids[split]] for split in ['ref', 'query']}\n",
    "\n",
    "    # compute edit distance approximation (eda) data and closest string retrival (csr) data\n",
    "    if compute_eda_data:\n",
    "        sequences, distances = edit_distance_approximation_data(eda_split_to_str_seqs, n_thread, alphabet, length)\n",
    "        save_as_pickle((sequences, distances), filenames[1])\n",
    "    if compute_csr_data:\n",
    "        references, queries, labels = closest_string_retrieval_data(csr_split_to_str_seqs, n_thread, alphabet, length)\n",
    "        save_as_pickle((references, queries, labels), filenames[2])\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_size = {'train': 7000, 'val': 100, 'test': 150, 'ref': 50000, 'query': 500}\n",
    "source_sequences = '../data/raw/greengenes/gg_13_5.fasta'\n",
    "n_thread = 5\n",
    "alphabet = ALPHABETS['DNA']\n",
    "outdir = '../data/interim/greengenes'\n",
    "\n",
    "filenames = main(split_to_size, source_sequences, alphabet, n_thread, outdir, compute_eda_data=False, compute_csr_data=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
