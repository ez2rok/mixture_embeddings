{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering ML Repo\n",
    "> Effect of embedding type and dimension on accuracy of clustering for classification tasks in ML Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clustering must allow custom distance metric\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "from util import cluster, mixture_embedding\n",
    "\n",
    "# Label tokens\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../../data/interim/mlrepo/sokol/task.txt'\n",
      "[Errno 2] No such file or directory: '../../data/interim/mlrepo/ravel/task.txt'\n",
      "name 'data_path' is not defined\n",
      "[Errno 2] No such file or directory: '../../data/interim/mlrepo/karlsson/gg/otutable.txt'\n",
      "[Errno 2] No such file or directory: '../../data/interim/mlrepo/gevers/task.txt'\n",
      "name 'data_path' is not defined\n",
      "[Errno 2] No such file or directory: '../../data/interim/mlrepo/qin2014/gg/otutable.txt'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 75\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Run experiment\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[39m# big_df = big_df.append(\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39m#     experiment(X, y, euc_embeddings, hyp_embeddings), \u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m#     ignore_index=True\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     out_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(\n\u001b[0;32m---> 75\u001b[0m         experiment(X, y, euc_embeddings, hyp_embeddings)\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m     big_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([big_df, out_df])\n\u001b[1;32m     78\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(X, y, euc_embeddings, hyp_embeddings)\u001b[0m\n\u001b[1;32m     23\u001b[0m X_pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39mhyp_dim)\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[1;32m     24\u001b[0m X_euc \u001b[39m=\u001b[39m mixture_embedding(X, euc_embeddings, geometry\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m X_hyp \u001b[39m=\u001b[39m mixture_embedding(X, hyp_embeddings, geometry\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhyperbolic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Cluster\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# out_df = pd.DataFrame(columns=[\"name\", \"dim\", \"type\", \"ARI\", \"accuracy\"])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m out \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/mixture_embeddings/notebooks/pac/util.py:55\u001b[0m, in \u001b[0;36mmixture_embedding\u001b[0;34m(otu_table, otu_embeddings, geometry, max_iter)\u001b[0m\n\u001b[1;32m     53\u001b[0m mixture_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_samples, n_dim))\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m i, sample \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(otu_table\u001b[39m.\u001b[39mindex):\n\u001b[0;32m---> 55\u001b[0m     mixture_embeddings[i] \u001b[39m=\u001b[39m fmean\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     56\u001b[0m         otu_embeddings, weights\u001b[39m=\u001b[39motu_table\u001b[39m.\u001b[39mloc[sample]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     57\u001b[0m     )\u001b[39m.\u001b[39mestimate_\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m mixture_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/mixture/lib/python3.11/site-packages/geomstats/learning/frechet_mean.py:554\u001b[0m, in \u001b[0;36mFrechetMean.fit\u001b[0;34m(self, X, y, weights)\u001b[0m\n\u001b[1;32m    551\u001b[0m     mean \u001b[39m=\u001b[39m linear_mean(points\u001b[39m=\u001b[39mX, weights\u001b[39m=\u001b[39mweights, point_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_type)\n\u001b[1;32m    553\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 554\u001b[0m     mean \u001b[39m=\u001b[39m _default_gradient_descent(\n\u001b[1;32m    555\u001b[0m         points\u001b[39m=\u001b[39mX,\n\u001b[1;32m    556\u001b[0m         weights\u001b[39m=\u001b[39mweights,\n\u001b[1;32m    557\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric,\n\u001b[1;32m    558\u001b[0m         max_iter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter,\n\u001b[1;32m    559\u001b[0m         init_step_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_step_size,\n\u001b[1;32m    560\u001b[0m         point_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_type,\n\u001b[1;32m    561\u001b[0m         epsilon\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon,\n\u001b[1;32m    562\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    563\u001b[0m         init_point\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_point,\n\u001b[1;32m    564\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39madaptive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    566\u001b[0m     mean \u001b[39m=\u001b[39m _adaptive_gradient_descent(\n\u001b[1;32m    567\u001b[0m         points\u001b[39m=\u001b[39mX,\n\u001b[1;32m    568\u001b[0m         metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    576\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mixture/lib/python3.11/site-packages/geomstats/learning/frechet_mean.py:125\u001b[0m, in \u001b[0;36m_default_gradient_descent\u001b[0;34m(points, metric, weights, max_iter, point_type, epsilon, init_step_size, verbose, init_point)\u001b[0m\n\u001b[1;32m    122\u001b[0m sq_dist \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    123\u001b[0m var \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m--> 125\u001b[0m norm_old \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(points)\n\u001b[1;32m    126\u001b[0m step \u001b[39m=\u001b[39m init_step_size\n\u001b[1;32m    128\u001b[0m \u001b[39mwhile\u001b[39;00m iteration \u001b[39m<\u001b[39m max_iter:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mixture/lib/python3.11/site-packages/numpy/linalg/linalg.py:2511\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2509\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[1;32m   2510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2511\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdot(x)\n\u001b[1;32m   2512\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_task(data_path, **embed_kwargs) -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Get OTU table\n",
    "    otu_path = f\"{data_path}/gg/otutable.txt\"\n",
    "    otu_table = pd.read_table(otu_path, index_col=0).T\n",
    "    X = otu_table / otu_table.values.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Tokenize labels\n",
    "    labels_path = f\"{data_path}/task.txt\"\n",
    "    labels = pd.read_table(labels_path, index_col=0)\n",
    "    label_encoder = OrdinalEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    y = label_encoder.transform(labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def experiment(X, y, euc_embeddings, hyp_embeddings):\n",
    "    \"\"\"For a dataset and a known dimensionality, get cluster scores\"\"\"\n",
    "    hyp_dim = hyp_embeddings.shape[1]\n",
    "    euc_dim = euc_embeddings.shape[1]\n",
    "    assert hyp_dim == euc_dim\n",
    "\n",
    "    X_raw = X.copy()\n",
    "    X_pca = PCA(n_components=hyp_dim).fit_transform(X)\n",
    "    X_euc = mixture_embedding(X, euc_embeddings, geometry=\"euclidean\")\n",
    "    X_hyp = mixture_embedding(X, hyp_embeddings, geometry=\"hyperbolic\")\n",
    "\n",
    "    # Cluster\n",
    "    # out_df = pd.DataFrame(columns=[\"name\", \"dim\", \"type\", \"ARI\", \"accuracy\"])\n",
    "    out = []\n",
    "    for X, name in zip([X_raw, X_pca, X_euc, X_hyp], [\"raw\", \"pca\", \"euc\", \"hyp\"]):\n",
    "        _, y_pred = cluster(X, n_clusters=2, labels=y)\n",
    "        out.append({\n",
    "            \"dim\": hyp_dim,\n",
    "            \"type\": name,\n",
    "            \"ARI\": adjusted_rand_score(y_pred, y),\n",
    "            \"accuracy\": accuracy_score(y_pred, y)\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "import os\n",
    "\n",
    "big_df = pd.DataFrame(columns=[\"name\", \"dim\", \"type\", \"ARI\", \"accuracy\"])\n",
    "for embed_dim in [16, 128]:\n",
    "    for dir in os.listdir(\"../../data/interim/mlrepo\"):\n",
    "        # Check it's a directory\n",
    "        if not os.path.isdir(f\"../../data/interim/mlrepo/{dir}\"):\n",
    "            continue\n",
    "\n",
    "        # Get data\n",
    "        try:\n",
    "            X, y = load_task(f\"../../data/interim/mlrepo/{dir}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Get embeddings\n",
    "        euc_embeddings = pd.read_csv(\n",
    "            f\"~/DATA/otu_embeddings/embeddings_euclidean_{embed_dim}.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "        hyp_embeddings = pd.read_csv(\n",
    "            f\"~/DATA/otu_embeddings/embeddings_hyperbolic_{embed_dim}.csv\", \n",
    "            index_col=0\n",
    "        )\n",
    "\n",
    "        # Run experiment\n",
    "        try:\n",
    "            # Pandas 2.0 does not support appending:\n",
    "            out_df = pd.concat(\n",
    "                experiment(X, y, euc_embeddings, hyp_embeddings)\n",
    "            )\n",
    "            big_df = pd.concat([big_df, out_df])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "big_df.to_csv(\"../../data/processed/mlrepo_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
