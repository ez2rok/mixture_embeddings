{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torchmetrics.classification.f_beta import FBetaScore\n",
    "\n",
    "from composer import Trainer, ComposerModel\n",
    "from composer.metrics import CrossEntropy\n",
    "\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class MLPModel(ComposerModel):\n",
    "    \"\"\"Implement MLP Classifier\"\"\"\n",
    "    def __init__(self, embedding_size, h1=64, h2=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # define model\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('input', nn.Linear(embedding_size, h1)),\n",
    "            ('sig1', nn.Sigmoid()),\n",
    "            ('hidden', nn.Linear(h1, h2)),\n",
    "            ('sig2', nn.Sigmoid()),\n",
    "            ('output', nn.Linear(h2, self.num_classes))\n",
    "        ]))\n",
    "        \n",
    "        # Metrics for training\n",
    "        self.train_metrics =  MetricCollection([\n",
    "            Accuracy(num_classes=self.num_classes, average='micro'),\n",
    "            FBetaScore(self.num_classes, mdmc_average='global')\n",
    "            ])\n",
    "\n",
    "        # Metrics for validation\n",
    "        self.val_metrics = MetricCollection([\n",
    "            CrossEntropy(),\n",
    "            Accuracy(num_classes=self.num_classes, average='micro'), \n",
    "            FBetaScore(self.num_classes, mdmc_average='global')\n",
    "            ])\n",
    " \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        inputs, _ = batch\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def loss(self, outputs, batch):\n",
    "        # pass batches and `forward` outputs to the loss\n",
    "        _, targets = batch\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    \n",
    "    def get_metrics(self, is_train=False):\n",
    "        if is_train:\n",
    "            metrics = self.train_metrics\n",
    "        else:\n",
    "            metrics = self.val_metrics\n",
    "\n",
    "        if isinstance(metrics, Metric):\n",
    "            metrics_dict = {metrics.__class__.__name__: metrics}\n",
    "        else:\n",
    "            metrics_dict = {}\n",
    "            for name, metric in metrics.items():\n",
    "                assert isinstance(metric, Metric)\n",
    "                metrics_dict[name] = metric\n",
    "\n",
    "        return metrics_dict\n",
    "\n",
    "    def update_metric(self, batch, outputs, metric: Metric) -> None:\n",
    "        _, targets = batch\n",
    "        metric.update(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, duration='100ep', h1=64, h2=32):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.duration = duration\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        \n",
    "        self.trainer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        # torch dataloader\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_train.astype(np.float32)),\n",
    "            torch.from_numpy(y_train)\n",
    "            )\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        # model\n",
    "        embedding_size = X_train.shape[1]\n",
    "        model = MLPModel(embedding_size, h1=self.h1, h2=self.h2, num_classes=self.num_classes)\n",
    "        self.model = model\n",
    "\n",
    "        # trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizers=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "            max_duration=self.duration,\n",
    "            device='gpu'\n",
    "        )\n",
    "        self.trainer = trainer\n",
    "        \n",
    "        # fit\n",
    "        self.trainer.fit()\n",
    "        \n",
    "        \n",
    "    def predict(self, X_test, y_test=None):\n",
    "        \n",
    "        # torch dataloader\n",
    "        if y_test is None:\n",
    "            y_test = np.zeros_like(X_test)\n",
    "        eval_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.astype(np.float32)), \n",
    "                                                      torch.from_numpy(y_test.astype(np.float32)))       \n",
    "        eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        # run predict\n",
    "        y_pred = self.trainer.predict(eval_dataloader)\n",
    "        y_pred = torch.vstack(tuple(y_pred))\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_107189/1771421923.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  y_train = y_train / np.random.randint(0, num_classes, size=(96))\n",
      "/tmp/ipykernel_107189/1771421923.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  y_test = y_test / np.random.randint(0, num_classes, size=(96))\n",
      "/tmp/ipykernel_107189/1771421923.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y_test = y_test / np.random.randint(0, num_classes, size=(96))\n"
     ]
    }
   ],
   "source": [
    "# define data\n",
    "num_classes = 8\n",
    "X_train, X_test = np.random.randint(0, 100, size=(96, 128)), np.random.randint(0, 100, size=(96, 128))\n",
    "y_train, y_test = np.random.randint(0, num_classes, size=(96)), np.random.randint(0, num_classes, size=(96))\n",
    "y_train = y_train / np.random.randint(0, num_classes, size=(96))\n",
    "y_test = y_test / np.random.randint(0, num_classes, size=(96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_classifier = MLP(num_classes, duration='10ep')\n",
    "# mlp_classifier.fit(X_train, y_train)\n",
    "# y_pred = mlp_classifier.predict(X_test)\n",
    "# y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.num_classes: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb Cell 6\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m mlp_regressor \u001b[39m=\u001b[39m MLP(num_classes, duration\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m10ep\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m mlp_regressor\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m y_pred \u001b[39m=\u001b[39m mlp_regressor\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;32m/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# trainer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     optimizers\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     max_duration\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduration,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m trainer\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baeolus/home/ethan/mixture_embeddings/src/classifiers/mlp_classifier.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# fit\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/mixture/lib/python3.10/site-packages/composer/trainer/trainer.py:977\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, train_dataloader, train_dataloader_label, train_subset_num_batches, max_duration, algorithms, algorithm_passes, optimizers, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_interval, eval_subset_num_batches, callbacks, loggers, run_name, progress_bar, log_to_console, console_stream, console_log_interval, log_traces, auto_log_hparams, load_path, load_object_store, load_weights_only, load_strict_model_weights, load_progress_bar, load_ignore_keys, load_exclude_algorithms, save_folder, save_filename, save_latest_filename, save_overwrite, save_interval, save_weights_only, save_num_checkpoints_to_keep, autoresume, deepspeed_config, fsdp_config, device, precision, grad_accum, device_train_microbatch_size, seed, deterministic_mode, dist_timeout, ddp_sync_strategy, profiler, python_log_level)\u001b[0m\n\u001b[1;32m    974\u001b[0m     prepare_fsdp_module(model, optimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfsdp_config, precision)\n\u001b[1;32m    976\u001b[0m \u001b[39m# Reproducibility\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m rank_zero_seed, seed \u001b[39m=\u001b[39m _distribute_and_get_random_seed(seed, device)\n\u001b[1;32m    978\u001b[0m \u001b[39m# If hparams is used to create the Trainer this function is called twice\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# which is okay because all runs with the hparams codepath will do this\u001b[39;00m\n\u001b[1;32m    980\u001b[0m reproducibility\u001b[39m.\u001b[39mseed_all(seed)\n",
      "File \u001b[0;32m~/mambaforge/envs/mixture/lib/python3.10/site-packages/composer/trainer/trainer.py:326\u001b[0m, in \u001b[0;36m_distribute_and_get_random_seed\u001b[0;34m(seed, device)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInvalid seed: \u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m. It must be on [0; 2**32 - 1)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    325\u001b[0m \u001b[39m# using int64 to prevent overflow\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m rank_zero_seed \u001b[39m=\u001b[39m device\u001b[39m.\u001b[39;49mtensor_to_device(torch\u001b[39m.\u001b[39;49mtensor([seed], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64))\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m dist\u001b[39m.\u001b[39mget_world_size() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    328\u001b[0m     dist\u001b[39m.\u001b[39mbroadcast(rank_zero_seed, src\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/mixture/lib/python3.10/site-packages/composer/devices/device_gpu.py:59\u001b[0m, in \u001b[0;36mDeviceGPU.tensor_to_device\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtensor_to_device\u001b[39m(\u001b[39mself\u001b[39m, tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device, non_blocking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "num_classes = 1\n",
    "mlp_regressor = MLP(num_classes, duration='10ep')\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "y_pred = mlp_regressor.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
